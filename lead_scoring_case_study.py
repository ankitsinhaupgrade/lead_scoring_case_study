# -*- coding: utf-8 -*-
"""Lead scoring case study

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lm3vNChJaN010oQkvAnzSruvRCV2p0-o

# Data cleaning
"""

#Warning supress
import warnings
warnings.filterwarnings('ignore')

#importing required library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#importing data
df=pd.read_csv('Leads.csv')

#checking top 5 records
df.head()

#checking shape of dataframe
df.shape

#checking information of dataframe like datatype and nulll
df.info()

#checking null data percentage
(df.isnull().sum()/df.shape[0])*100

#deleting columns have 40 % null
drop_column_list=[]
for i in df.columns:
   if (df[i].isnull().mean())*100>35:
    drop_column_list.append(i)
print(drop_column_list)

#droping columns from dataframe
for i in drop_column_list:
  df.drop(i,axis=1,inplace=True)

#Checing shape
df.shape

#checking rest null values
(df.isnull().mean())*100

#dealing with null columns having null % more than 5
null_columns_5pect=[]
for i in df.columns:
  if (df[i].isnull().mean())*100>5:
    null_columns_5pect.append(i)
print(null_columns_5pect)

#dealing with null columns replacing with 'Unknown'
for i in null_columns_5pect:
  df[i]=df[i].replace(np.NaN,'Unknown')

#finding pect of null for rest column
(df.isnull().mean())*100

#dropping rows which have/has null, they are nearly 1%
#checking shape before drop
print(df.shape)
#dropping null rows
df=df.dropna()
#checking shape after drop
print(df.shape)
#very less drop

#finally checking again the null values
(df.isnull().mean())*100

"""No null and data cleaning done. Good to proceed for EDA

# EDA

Univariate Analysis and Bivariate Analysis
"""

#Converted is a target variable.
#lets try to understand it.The target variable. Indicates whether a lead has been successfully converted or not."1" for success and "0" for failure
df["Converted"].unique()

#percentage of converted
Converted_pect = (sum(df['Converted'])/len(df['Converted'].index))*100
Converted_pect

#Check the origin of lead
sns.countplot(x = "Lead Origin", hue = "Converted", data = df,palette='Set1')
plt.xticks(rotation=45)
plt.show()

"""

*   Major traffic comes from API and Landing page
*   If traffic comes from lead add, it will be converted successfully most of time.


*   Seems like if we concentrate on API and Landing page submission people, It will increase lead converson.



"""

#Checking "Lead Source"
sns.countplot(x = "Lead Source", hue = "Converted", data = df, palette='Set1')
plt.xticks(rotation = 90)
plt.show()

#  'google' and 'Google' are same so replacing
df['Lead Source'] = df['Lead Source'].replace(['google'], 'Google')
# Creating a new category 'Others' for some of the Lead Sources which do not have much values.
df['Lead Source'] = df['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',
  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Others')

#Checking "Lead Source"
sns.countplot(x = "Lead Source", hue = "Converted", data = df, palette='Set1')
plt.xticks(rotation = 90)
plt.show()

"""

*   Olark chat,organic seach,direct traffic and google are major source
*   Traffic comming from Reference has conversion rate very high


*   Concentrate on top 4 for more conversion rate



"""

#Total Time Spent on Website
sns.boxplot(df['Total Time Spent on Website'])
plt.show()

sns.boxplot(y = 'Total Time Spent on Website', x = 'Converted', data = df)
plt.show()

"""

*   Leads spending more time on the weblise are more likely to be converted.


*   Website should be made more engaging to make leads spend more time."""

#Last Activity
plt.figure(figsize=(15,6))
sns.countplot(x = "Last Activity", hue = "Converted", data = df)
plt.xticks(rotation = 90)
plt.show()

"""

*   Sms sent are mostly converted
*   Need to concentrate on Email opened because it is very high in number and conversion is not good



"""

#What is your current occupation
plt.figure(figsize=(15,6))
sns.countplot(x = "What is your current occupation", hue = "Converted", data = df)
plt.xticks(rotation = 90)
plt.show()

"""

*   Unemployed people are mostly not converted.May be due to cost of cource
*   Working professionals have good chance to convert.
* There are unknown people which we has not fill the information of occupation are mostly not converted

"""

#Country
plt.figure(figsize=(15,6))
sns.countplot(x = "Country", hue = "Converted", data = df)
plt.xticks(rotation = 90)
plt.show()

"""Most of people appling for course is from India"""

#Search
plt.figure(figsize=(15,6))
sns.countplot(x = "Search", hue = "Converted", data = df)
plt.xticks(rotation = 90)
plt.show()

"""Not so clear ablout search"""

#dropping unneccessary columns
df = df.drop(['Lead Number','Country','Search','Magazine','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses',
              'Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque','A free copy of Mastering The Interview'],axis=1)

#checking shape
df.shape

df.info()

#preparing data for label encoding
#finding object columns
df_cols=df.loc[:,df.dtypes=='object']
df_cols.columns

#lebel encoding
from sklearn import preprocessing
lebelencoder=preprocessing.LabelEncoder()
for i in df_cols:
  df[i]=lebelencoder.fit_transform(df[i])

df.head()

#Droping Prospect ID, it is id
df=df.drop(['Prospect ID'],axis=1)
df.head()

#Splitting the data into train and test set.
from sklearn.model_selection import train_test_split

# Putting feature variable to X
X = df.drop(['Converted'], axis=1)
X.head()

# Putting target variable to y
y = df['Converted']

y.head()

# Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)

#Scaling feature
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])

X_train.head()

# Checking the Lead Conversion rate
Converted = (sum(df['Converted'])/len(df['Converted'].index))*100
Converted

"""RFE Feature Selection"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

from sklearn.feature_selection import RFE
rfe = RFE(estimator=logreg, n_features_to_select=15)             # running RFE with 15 variables as output
rfe = rfe.fit(X_train, y_train)

#columns selected
list(zip(X_train.columns, rfe.support_, rfe.ranking_))

# Viewing columns selected by RFE
cols = X_train.columns[rfe.support_]
cols

"""# Model building

Model 1
"""

import statsmodels.api as sm

X_train_sm = sm.add_constant(X_train[cols])
logm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
result = logm1.fit()
result.summary()

"""Highet p value for Do Not Call."""

#dropping Do Not Call
col1=cols.drop("Do Not Call")

"""Model 2"""

X_train_sm = sm.add_constant(X_train[col1])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

"""Highet p value for City."""

col2=col1.drop("City")

"""Model 3"""

X_train_sm = sm.add_constant(X_train[col2])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

"""P values look good."""

# Check for the VIF values of the feature variables.
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train[col2].columns
vif['VIF'] = [variance_inflation_factor(X_train[col2].values, i) for i in range(X_train[col2].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

#high vif of Lead Profile
col3=col2.drop("Lead Profile")

"""Model 4"""

X_train_sm = sm.add_constant(X_train[col3])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Check for the VIF values of the feature variables.
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train[col3].columns
vif['VIF'] = [variance_inflation_factor(X_train[col3].values, i) for i in range(X_train[col3].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

#Removing What is your current occupation for high vif
col3=col3.drop("What is your current occupation")

"""Model 5"""

X_train_sm = sm.add_constant(X_train[col3])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Check for the VIF values of the feature variables.
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train[col3].columns
vif['VIF'] = [variance_inflation_factor(X_train[col3].values, i) for i in range(X_train[col3].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

#removing Last Notable Activity for high vif
col3=col3.drop("Last Notable Activity")

"""Model 6"""

X_train_sm = sm.add_constant(X_train[col3])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Check for the VIF values of the feature variables.
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train[col3].columns
vif['VIF'] = [variance_inflation_factor(X_train[col3].values, i) for i in range(X_train[col3].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

#dropping How did you hear about X Education due to high vif
col3=col3.drop("How did you hear about X Education")

"""Model 7"""

X_train_sm = sm.add_constant(X_train[col3])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Check for the VIF values of the feature variables.
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train[col3].columns
vif['VIF'] = [variance_inflation_factor(X_train[col3].values, i) for i in range(X_train[col3].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""Since the Pvalues of all variables is approx 0 and VIF values are low for all the variables, model 7 is our final model. We have 9 variables in our final model.

Making Prediction on the Train set
"""

# Getting the predicted values on the train set
y_train_pred = res.predict(X_train_sm)
y_train_pred[:10]

# Reshaping into an array
y_train_pred = y_train_pred.values.reshape(-1)
y_train_pred[:10]

"""Creating a dataframe with the actual Converted flag and the predicted probabilities"""

y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})
y_train_pred_final['Prospect ID'] = y_train.index
y_train_pred_final.head()

"""Choosing an arbitrary cut-off probability point of 0.5 to find the predicted labelsÂ¶
Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0
"""

y_train_pred_final['predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)

# Let's see the head
y_train_pred_final.head()

"""Making the Confusion matrix"""

from sklearn import metrics

# Confusion matrix
confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )
print(confusion)

# Let's check the overall accuracy.
print('Accuracy :',metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

"""Metrics beyond simply accuracy"""

TP = confusion[1,1] # true positive
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

# Sensitivity of our logistic regression model
print("Sensitivity : ",TP / float(TP+FN))

# Let us calculate specificity
print("Specificity : ",TN / float(TN+FP))

# Calculate false postive rate - predicting converted lead when the lead actually was not converted
print("False Positive Rate :",FP/ float(TN+FP))

# positive predictive value
print("Positive Predictive Value :",TP / float(TP+FP))

# Negative predictive value
print ("Negative predictive value :",TN / float(TN+ FN))

"""We found out that our specificity was good (~86%) but our sensitivity was only 64%. Hence, this needed to be taken care of.

We have got sensitivity of 64% and this was mainly because of the cut-off point of 0.5 that we had arbitrarily chosen. Now, this cut-off point had to be optimised in order to get a decent value of sensitivity and for this we will use the ROC curve.

Plotting the ROC Curve
"""

def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(5, 5))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    return None

fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_prob, drop_intermediate = False )

draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)

"""Since we have higher (0.85) area under the ROC curve , therefore our model is a good one."""

# Let's create columns with different probability cutoffs
numbers = [float(x)/10 for x in range(10)]
for i in numbers:
    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)
y_train_pred_final.head()

# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.
cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])
from sklearn.metrics import confusion_matrix

# TP = confusion[1,1] # true positive
# TN = confusion[0,0] # true negatives
# FP = confusion[0,1] # false positives
# FN = confusion[1,0] # false negatives

num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
for i in num:
    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )
    total1=sum(sum(cm1))
    accuracy = (cm1[0,0]+cm1[1,1])/total1

    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])
    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]
print(cutoff_df)

# Let's plot accuracy sensitivity and specificity for various probabilities.
cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])
plt.show()

"""From the curve above, 0.38 is the optimum point to take it as a cutoff probability."""

y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.38 else 0)

y_train_pred_final.head()

"""Assigning Lead Score to the Training data"""

y_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_prob.map( lambda x: round(x*100))

y_train_pred_final.head()

#Model evalution

# Let's check the overall accuracy.
print("Accuracy :",metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted))

# Confusion matrix
confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )
confusion2

TP = confusion2[1,1] # true positive
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

# Let's see the sensitivity of our logistic regression model
print("Sensitivity : ",TP / float(TP+FN))

# Let us calculate specificity
print("Specificity :",TN / float(TN+FP))

# Calculate false postive rate - predicting converted lead when the lead was actually not have converted
print("False Positive rate : ",FP/ float(TN+FP))

# Positive predictive value
print("Positive Predictive Value :",TP / float(TP+FP))

# Negative predictive value
print("Negative Predictive Value : ",TN / float(TN+ FN))

"""# **Precision and Recall**"""

#Looking at the confusion matrix again

confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )
confusion

# Precision
TP / TP + FP

print("Precision : ",confusion[1,1]/(confusion[0,1]+confusion[1,1]))

# Recall
TP / TP + FN

print("Recall :",confusion[1,1]/(confusion[1,0]+confusion[1,1]))

#Precision and recall tradeoff
from sklearn.metrics import precision_recall_curve
y_train_pred_final.Converted, y_train_pred_final.predicted

p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)

# plotting a trade-off curve between precision and recall
plt.plot(thresholds, p[:-1], "g-")
plt.plot(thresholds, r[:-1], "r-")
plt.show()

"""# Making predictions on the test set"""

X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits',
                                                                                                        'Total Time Spent on Website',
                                                                                                        'Page Views Per Visit']])

# Assigning the columns selected by the final model to the X_test
X_test = X_test[col3]
X_test.head()

# Adding a const
X_test_sm = sm.add_constant(X_test)

# Making predictions on the test set
y_test_pred = res.predict(X_test_sm)
y_test_pred[:10]

# Converting y_test_pred to a dataframe which is an array
y_pred_1 = pd.DataFrame(y_test_pred)

# Let's see the head
y_pred_1.head()

# Converting y_test to dataframe
y_test_df = pd.DataFrame(y_test)

# Putting Prospect ID to index
y_test_df['Prospect ID'] = y_test_df.index

# Removing index for both dataframes to append them side by side
y_pred_1.reset_index(drop=True, inplace=True)
y_test_df.reset_index(drop=True, inplace=True)

# Appending y_test_df and y_pred_1
y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)

y_pred_final.head()

# Assigning the columns selected by the final model to the X_test
X_test = X_test[col3]
X_test.head()

# Adding a const
X_test_sm = sm.add_constant(X_test)

# Making predictions on the test set
y_test_pred = res.predict(X_test_sm)
y_test_pred[:10]

# Converting y_test_pred to a dataframe which is an array
y_pred_1 = pd.DataFrame(y_test_pred)

# Let's see the head
y_pred_1.head()

# Converting y_test to dataframe
y_test_df = pd.DataFrame(y_test)

# Putting Prospect ID to index
y_test_df['Prospect ID'] = y_test_df.index

# Removing index for both dataframes to append them side by side
y_pred_1.reset_index(drop=True, inplace=True)
y_test_df.reset_index(drop=True, inplace=True)

# Appending y_test_df and y_pred_1
y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)

y_pred_final.head()

# Renaming the column
y_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_prob'})

# Rearranging the columns
y_pred_final = y_pred_final.reindex(columns=['Prospect ID','Converted','Converted_prob'])

# Let's see the head of y_pred_final
y_pred_final.head()

y_pred_final['final_predicted'] = y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.34 else 0)

y_pred_final.head()

# Let's check the overall accuracy.
print("Accuracy :",metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted))

# Making the confusion matrix
confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )
confusion2

TP = confusion2[1,1] # true positive
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

# Let's see the sensitivity of our logistic regression model
print("Sensitivity :",TP / float(TP+FN))

# Let us calculate specificity
print("Specificity :",TN / float(TN+FP))

#Assigning Lead Score to the Testing data

y_pred_final['Lead_Score'] = y_pred_final.Converted_prob.map( lambda x: round(x*100))

y_pred_final.head()

"""# Observations
Accuracy : 74.8%

Sensitivity : 78.8 %

Specificity : 72.4 %

This is close to train data. We are good.

# Results
1) Comparing the values obtained for Train & Test:
Train:
Accuracy : 77.4%
Sensitivity : 77.2 %
Specificity : 77.5 %

Test:
Accuracy : 77.8%
Sensitivity : 78.8 %
Specificity : 72.4 %
Thus we have achieved our goal of getting a ballpark of the target lead conversion rate to be around 77% . The Model seems to predict the Conversion Rate very well and we should be able to give the CEO confidence in making good calls based on this model to get a higher lead conversion rate of 77%.

2) Finding out the leads which should be contacted:
"""

#The customers which should be contacted are the customers whose "Lead Score" is equal to or greater than 85. They can be termed as 'Hot Leads'.
hot_leads=y_pred_final.loc[y_pred_final["Lead_Score"]>=85]
hot_leads

print("The Prospect ID of the customers which should be contacted are :")

hot_leads_ids = hot_leads["Prospect ID"].values.reshape(-1)
hot_leads_ids

"""3) Finding out the Important Features from our final model:"""

res.params.sort_values(ascending=False)











